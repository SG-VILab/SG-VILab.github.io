[{"authors":null,"categories":null,"content":"Prof Lu received his PhD in electrical and computer engineering from the National University of Singapore. His research interests include computer vision and deep learning. He has published more than 100 internationally refereed journal and conference papers and co-authored over 10 patents in these research areas. Before joining in NTU, he took a number of leadership roles in the Institute for Infocomm Research (I2R), under the Agency for Science, Technology, and Research (A*SATR) in Singapore, including Head of Visual Attention Lab, Deputy Head of Satellite Department, Co-Chair of the Image and Pervasive Access Laboratory (a CNRS overseas laboratory hosted by A*STAR in Singapore). Dr Lu is currently an Associate Editor for the journals of Pattern Recognition (PR) and Neurocomputing. He has also served in the program committee of a number of international conferences such as the Senior Program Committee of the International Joint Conferences on Artificial Intelligence (IJCAI) and AAAI Conference on Artificial Intelligence (AAAI) from 2018 to 2021, the General Chair of the IAPR International Workshop on Document Analysis System (DAS) in 2020, etc.\nCommittee Members:\n Senior Program Committee (SPC) of the International Joint Conference on Artificial Intelligence (IJCAI18 – IJCAI21) Senior Program Committee (SPC) of the AAAI Conference on Artificial Intelligence (AAAI18 – AAAI21) Area Chair of 15th International Conference on Document Analysis and Recognition (ICDAR-19) Area Chair of IEEE Winter Conference on Applications of Computer Vision (WACV-17 and WACV-18) Area Chair of 14th IAPR International Conference on Document Analysis and Recognition (ICDAR-17)  Awards and Recognitions:\n Top winner of the ICFHR2014 Competition on Word Recognition from Historical Documents Top winner of the ICDAR 2013 Robust Reading Competition (1st in the scene text segmentation task) Top winner of the ICDAR 2013 Document Image Binarization Contest (DIBCO 2013) Top winner of the H-DIBCO 2010 – Handwritten Document Image Binarization Competition Top winner of the ICDAR 2009 Document Image Binarization Contest (DIBCO 2009)  Research interests:\n Image and video analytics Visual intelligence Cognitive vision Machine learning  Research topics:\n Scene text detection and recognition Unsupervised domain adaptation Image synthesis Satellite image analytics Facial expression classification and generation  Research projects:\n Unsupervised Domain Adaptation for Scalable Deep Learning High-Fidelity Image Synthesis for Effective and Efficient and Efficient Deep Network Training A Semi-Supervised Learning Approach for Accurate and Robust Detection of Texts in Scenes Weakly and Semi-Supervised Deep Network Learning for Accurate and Robust Object Detection and Recognition Robust and Accurate Facial Expression Recognition for Game Controls on Smartphones  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Prof Lu received his PhD in electrical and computer engineering from the National University of Singapore. His research interests include computer vision and deep learning. He has published more than 100 internationally refereed journal and conference papers and co-authored over 10 patents in these research areas.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e29cb902b5fdc150848793b9e88bf03c","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4f35d98ea71af34b146c3c96c15b541c","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d41e7147d34449fc40e43469e10ccc1d","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"00d3a4ab85b58e74b14e6744c7b8119f","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"66b3eb90304b33f1c3de3db2cb2abe82","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Jiahui Zhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e51c2ef2241bc9a10ca42fecf16cf288","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"473d8063261d05d17a34180c910206f2","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b401295fe1f620edbc7c27fc72b783f6","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bbba0c85cf67c249a51e0b4e60d95cdd","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"daf718f568d0a7bd63a08ed53fe79b6c","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d52d98bd07dd132313a071afb03cfb98","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"94284a75185f2c4989ca5250b7c7e328","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"26ddc454086775af6331b55c2876388c","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ebcafd603d2ec572e341342f64e0e301","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Aoran Xiao","Jiaxing Huang","Dayan Guan","Fangneng Zhan","Shijian Lu"],"categories":null,"content":"","date":1627138800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627138800,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://sg-vilab.github.io/event/example/","publishdate":"2021-07-24T15:00:00Z","relpermalink":"/event/example/","section":"event","summary":"SynLiDAR dataset","tags":[],"title":"SynLiDAR dataset: Learning From Synthetic LiDAR Sequential Point Cloud","type":"event"},{"authors":["Dayan Guan","Jiaxing Huang","Aoran Xiao","Shijian Lu"],"categories":null,"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"9c81815dd07ad01e71125c2e149b886d","permalink":"https://sg-vilab.github.io/publication/guan2021domain/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/publication/guan2021domain/","section":"publication","summary":"Video semantic segmentation is an essential task for the analysis and understanding of videos. Recent efforts largely focus on supervised video segmentation by learning from fully annotated data, but the learnt models often experience clear performance drop while applied to videos of a different domain. This paper presents DA-VSN, a domain adaptive video segmentation network that addresses domain gaps in videos by temporal consistency regularization (TCR) for consecutive frames of target-domain videos. DA-VSN consists of two novel and complementary designs. The first is cross-domain TCR that guides the prediction of target frames to have similar temporal consistency as that of source frames (learnt from annotated source data) via adversarial learning. The second is intra-domain TCR that guides unconfident predictions of target frames to have similar temporal consistency as confident predictions of target frames. Extensive experiments demonstrate the superiority of our proposed domain adaptive video segmentation network which outperforms multiple baselines consistently by large margins.","tags":null,"title":"Domain Adaptive Video Segmentation via Temporal Consistency Regularization","type":"publication"},{"authors":null,"categories":null,"content":" Domain Adaptive Video Segmentation via Temporal Consistency Regularization Coming soon ","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"771403a29a897956a3be912e07439717","permalink":"https://sg-vilab.github.io/post/iccv2021/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/post/iccv2021/","section":"post","summary":" Domain Adaptive Video Segmentation via Temporal Consistency Regularization Coming soon ","tags":null,"title":"Six Papers Accepted by ICCV 2021","type":"post"},{"authors":["Jiaxing Huang","Dayan Guan","Aoran Xiao","Shijian Lu"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"0905fd9837fda88078d2931cf42d8b07","permalink":"https://sg-vilab.github.io/publication/huang2021cross/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/huang2021cross/","section":"publication","summary":"Panoptic segmentation unifies semantic segmentation and instance segmentation which has been attracting increasing attention in recent years. However, most existing research was conducted under a supervised learning setup whereas unsupervised domain adaptive panoptic segmentation which is critical in different tasks and applications is largely neglected. We design a domain adaptive panoptic segmentation network that exploits inter-style consistency and inter-task regularization for optimal domain adaptive panoptic segmentation. The inter-style consistency leverages geometric invariance across the same image of the different styles which fabricates certain self-supervisions to guide the network to learn domain-invariant features. The inter-task regularization exploits the complementary nature of instance segmentation and semantic segmentation and uses it as a constraint for better feature alignment across domains. Extensive experiments over multiple domain adaptive panoptic segmentation tasks (e.g., synthetic-to-real and real-to-real) show that our proposed network achieves superior segmentation performance as compared with the state-of-the-art.","tags":null,"title":"Cross-view regularization for domain adaptive panoptic segmentation","type":"publication"},{"authors":["Jiaxing Huang","Dayan Guan","Aoran Xiao","Shijian Lu"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"caa62cdf07f79542450b16d9ed2abd30","permalink":"https://sg-vilab.github.io/publication/huang2021fsdr/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/huang2021fsdr/","section":"publication","summary":"Domain generalization aims to learn a generalizable model from a known source domain for various unknown target domains. It has been studied widely by domain randomization that transfers source images to different styles in spatial space for learning domain-agnostic features. However, most existing randomization uses GANs that often lack of controls and even alter semantic structures of images undesirably. Inspired by the idea of JPEG that converts spatial images into multiple frequency components (FCs), we propose Frequency Space Domain Randomization (FSDR) that randomizes images in frequency space by keeping domain-invariant FCs (DIFs) and randomizing domain-variant FCs (DVFs) only. FSDR has two unique features':' 1) it decomposes images into DIFs and DVFs which allows explicit access and manipulation of them and more controllable randomization; 2) it has minimal effects on semantic structures of images and domain-invariant features. We examined domain variance and invariance property of FCs statistically and designed a network that can identify and fuse DIFs and DVFs dynamically through iterative learning. Extensive experiments over multiple domain generalizable segmentation tasks show that FSDR achieves superior segmentation and its performance is even on par with domain adaptation methods that access target data in training.","tags":null,"title":"FSDR: Frequency Space Domain Randomization for Domain Generalization","type":"publication"},{"authors":null,"categories":null,"content":" Cross-view regularization for domain adaptive panoptic segmentation [oral]  FSDR: Frequency Space Domain Randomization for Domain Generalization  Unbalanced Feature Transport for Exemplar-based Image Translation ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"5bff8d7ad55ab554b46cacc748d53173","permalink":"https://sg-vilab.github.io/post/cvpr2021/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/post/cvpr2021/","section":"post","summary":" Cross-view regularization for domain adaptive panoptic segmentation [oral]  FSDR: Frequency Space Domain Randomization for Domain Generalization  Unbalanced Feature Transport for Exemplar-based Image Translation ","tags":null,"title":"Three Papers (One Oral) Accepted by CVPR 2021","type":"post"},{"authors":["Fangneng Zhan","Yingchen Yu","Kaiwen Cui","Gongjie Zhang","Shijian Lu","Jianxiong Pan","Changgong Zhang","Feiying Ma","Xuansong Xie","Chunyan Miao"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"439d341e91be290336641d0801841aa7","permalink":"https://sg-vilab.github.io/publication/zhan2021unbalanced/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/zhan2021unbalanced/","section":"publication","summary":"Despite the great success of GANs in images translation with different conditioned inputs such as semantic segmentation and edge maps, generating high-fidelity realistic images with reference styles remains a grand challenge in conditional image-to-image translation. This paper presents a general image translation framework that incorporates optimal transport for feature alignment between conditional inputs and style exemplars in image translation. The introduction of optimal transport mitigates the constraint of many-to-one feature matching significantly while building up accurate semantic correspondences between conditional inputs and exemplars. We design a novel unbalanced optimal transport to address the transport between features with deviational distributions which exists widely between conditional inputs and exemplars. In addition, we design a semantic-activation normalization scheme that injects style features of exemplars into the image translation process successfully. Extensive experiments over multiple image translation tasks show that our method achieves superior image translation qualitatively and quantitatively as compared with the state-of-the-art.","tags":null,"title":"Unbalanced Feature Transport for Exemplar-based Image Translation","type":"publication"},{"authors":["Zongxian Li","Qixiang Ye","Chong Zhang","Jingjing Liu","Shijian Lu","Yonghong Tian"],"categories":null,"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"3cbdd445a9942c5e7703969cc7ba8f5e","permalink":"https://sg-vilab.github.io/publication/zhang2021self/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/publication/zhang2021self/","section":"publication","summary":"Unsupervised domain adaptation (UDA) has achieved unprecedented success in improving the cross-domain robustness of object detection models. However, existing UDA methods largely ignore the instantaneous data distribution during model learning, which could deteriorate the feature representation given large domain shift. In this work, we propose a Self-Guided Adaptation (SGA) model, target at aligning feature representation and transferring object detection models across domains while considering the instantaneous alignment difficulty. The core of SGA is to calculate \"hardness\" factors for sample pairs indicating domain distance in a kernel space. With the hardness factor, the proposed SGA adaptively indicates the importance of samples and assigns them different constrains. Indicated by hardness factors, Self-Guided Progressive Sampling (SPS) is implemented in an \"easy-to-hard\" way during model adaptation. Using multi-stage convolutional features, SGA is further aggregated to fully align hierarchical representations of detection models. Extensive experiments on commonly used benchmarks show that SGA improves the state-of-the-art methods with significant margins, while demonstrating the effectiveness on large domain shift.","tags":null,"title":"Self-Guided Adaptation: Progressive Representation Alignment for Domain Adaptive Object Detection","type":"publication"},{"authors":null,"categories":null,"content":" Self-Guided Adaptation: Progressive Representation Alignment for Domain Adaptive Object Detection Uncertainty-aware unsupervised domain adaptation in object detection ","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"18d7a01e09e58c7611fb222971312b29","permalink":"https://sg-vilab.github.io/post/tmm2021/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/post/tmm2021/","section":"post","summary":" Self-Guided Adaptation: Progressive Representation Alignment for Domain Adaptive Object Detection Uncertainty-aware unsupervised domain adaptation in object detection ","tags":null,"title":"Two Papers Accepted by TMM 2021","type":"post"},{"authors":["Dayan Guan","Jiaxing Huang","Aoran Xiao","Shijian Lu","Yanpeng Cao"],"categories":null,"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"b80cbf82031d90b6ac336f31230e4bb5","permalink":"https://sg-vilab.github.io/publication/guan2021uncertainty/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/publication/guan2021uncertainty/","section":"publication","summary":"Unsupervised domain adaptive object detection aims to adapt detectors from a labelled source domain to an unlabelled target domain. Most existing works take a two-stage strategy that first generates region proposals and then detects objects of interest, where adversarial learning is widely adopted to mitigate the inter-domain discrepancy in both stages. However, adversarial learning may impair the alignment of well-aligned samples as it merely aligns the global distributions across domains. To address this issue, we design an uncertainty-aware domain adaptation network (UaDAN) that introduces conditional adversarial learning to align well-aligned and poorly-aligned samples separately in different manners. Specifically, we design an uncertainty metric that assesses the alignment of each sample and adjusts the strength of adversarial learning for well-aligned and poorly-aligned samples adaptively. In addition, we exploit the uncertainty metric to achieve curriculum learning that first performs easier image-level alignment and then more difficult instance-level alignment progressively. Extensive experiments over four challenging domain adaptive object detection datasets show that UaDAN achieves superior performance as compared with state-of-the-art methods.","tags":null,"title":"Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection","type":"publication"},{"authors":null,"categories":null,"content":" Scale variance minimization for unsupervised domain adaptation in image segmentation ","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"b95d868447f90631a030f5759603a498","permalink":"https://sg-vilab.github.io/post/pr2021/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/post/pr2021/","section":"post","summary":" Scale variance minimization for unsupervised domain adaptation in image segmentation ","tags":null,"title":"One Paper Accepted by Pattern Recognition 2021","type":"post"},{"authors":["Dayan Guan","Jiaxing Huang","Shijian Lu","Aoran Xiao"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"9534379289e06a03d37300cc3045efcf","permalink":"https://sg-vilab.github.io/publication/guan2021scale/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/guan2021scale/","section":"publication","summary":"We focus on unsupervised domain adaptation (UDA) in image segmentation. Existing works address this challenge largely by aligning inter-domain representations, which may lead over-alignment that impairs the semantic structures of images and further target-domain segmentation performance. We design a scale variance minimization (SVMin) method by enforcing the intra-image semantic structure consistency in the target domain. Specifically, SVMin leverages an intrinsic property that simple scale transformation has little effect on the semantic structures of images. It thus introduces certain supervision in the target domain by imposing a scale-invariance constraint while learning to segment an image and its scale-transformation concurrently. Additionally, SVMin is complementary to most existing UDA techniques and can be easily incorporated with consistent performance boost but little extra parameters. Extensive experiments show that our method achieves superior domain adaptive segmentation performance as compared with the state-of-the-art. Preliminary studies show that SVMin can be easily adapted for UDA-based image classification.","tags":null,"title":"Scale variance minimization for unsupervised domain adaptation in image segmentation","type":"publication"},{"authors":null,"categories":null,"content":" Visual Navigation With Multiple Goals Based on Deep Reinforcement Learning  ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"89e090b981c0fb05772e636b78a520a8","permalink":"https://sg-vilab.github.io/post/tnnls2021/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/post/tnnls2021/","section":"post","summary":" Visual Navigation With Multiple Goals Based on Deep Reinforcement Learning  ","tags":null,"title":"One Paper Accepted by TNNLS 2021","type":"post"},{"authors":["Zhenhuan Rao","Yuechen Wu","Zifei Yang","Wei Zhang","Shijian Lu","Weizhi Lu","ZhengJun Zha"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"d1c64b4b73488705f371e484029d9470","permalink":"https://sg-vilab.github.io/publication/rao2021visual/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/rao2021visual/","section":"publication","summary":"Learning to adapt to a series of different goals in visual navigation is challenging. In this work, we present a model-embedded actor-critic architecture for the multigoal visual navigation task. To enhance the task cooperation in multigoal learning, we introduce two new designs to the reinforcement learning scheme':' inverse dynamics model (InvDM) and multigoal colearning (MgCl). Specifically, InvDM is proposed to capture the navigation-relevant association between state and goal and provide additional training signals to relieve the sparse reward issue. MgCl aims at improving the sample efficiency and supports the agent to learn from unintentional positive experiences. Besides, to further improve the scene generalization capability of the agent, we present an enhanced navigation model that consists of two self-supervised auxiliary task modules. The first module, which is named path closed-loop detection, helps to understand whether the state has been experienced. The second one, namely the state-target matching module, tries to figure out the difference between state and goal. Extensive results on the interactive platform AI2-THOR demonstrate that the agent trained with the proposed method converges faster than state-of-the-art methods while owning good generalization capability. The video demonstration is available at https://vsislab.github.io/mgvn.","tags":null,"title":"Visual Navigation With Multiple Goals Based on Deep Reinforcement Learning","type":"publication"},{"authors":["Chun-Mei Feng","Kai Wang","Shijian Lu","Yong Xu","Xuelong Li"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"14f9f6158b07d010c54c171bfb2ed7fe","permalink":"https://sg-vilab.github.io/publication/feng2021brain/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/feng2021brain/","section":"publication","summary":"Magnetic Resonance Imaging (MRI) has been widely used in clinical application and pathology research to help doctors provide better diagnoses. However, accurate diagnosis by MRI remains a great challenge, as images obtained via current MRI techniques usually have low resolutions. Improving MRI image quality and resolution has thus become a critically important task. This paper presents an innovative Coupled-Projection Residual Network (CPRN) for MRI super-resolution. CPRN consists of two complementary sub-networks':' a shallow network and a deep one, which maintain content consistency while learning high frequency differences between low-resolution and high-resolution images. The shallow sub-network employs coupled-projection to better retain the MR image details, where a novel feedback mechanism is introduced to guide the reconstruction of high-resolution images. The deep sub-network learns from the residuals of the high-frequency image information, where multiple residual blocks are cascaded to magnify the MR images at the last network layer. Finally, the features from the shallow and deep sub-networks are fused for the reconstruction of high-resolution MR images. For effective feature fusion between the deep and shallow sub-networks, a step-wise connection (CPRN_S) is designed, inspired by the human cognitive process (from simple to complex). Experiments over three public MRI datasets show that our proposed CPRN achieves superior MRI super-resolution performance compared with the state-of-the-art.","tags":null,"title":"Brain MRI super-resolution using coupled-projection residual network","type":"publication"},{"authors":["Fangneng Zhan","Changgong Zhang","Yingchen Yu","Yuan Chang","Shijian Lu","Feiying Ma","Xuansong Xie"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"83385039a4cd907eec4ef23f9550bbbb","permalink":"https://sg-vilab.github.io/publication/zhan2020emlight/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/zhan2020emlight/","section":"publication","summary":"Illumination estimation from a single image is critical in 3D rendering and it has been investigated extensively in the computer vision and computer graphic research community. On the other hand, existing works estimate illumination by either regressing light parameters or generating illumination maps that are often hard to optimize or tend to produce inaccurate predictions. We propose Earth Mover Light (EMLight), an illumination estimation framework that leverages a regression network and a neural projector for accurate illumination estimation. We decompose the illumination map into spherical light distribution, light intensity and the ambient term, and define the illumination estimation as a parameter regression task for the three illumination components. Motivated by the Earth Mover distance, we design a novel spherical mover's loss that guides to regress light distribution parameters accurately by taking advantage of the subtleties of spherical distribution. Under the guidance of the predicted spherical distribution, light intensity and ambient term, the neural projector synthesizes panoramic illumination maps with realistic light frequency. Extensive experiments show that EMLight achieves accurate illumination estimation and the generated relighting in 3D object embedding exhibits superior plausibility and fidelity as compared with state-of-the-art methods.","tags":null,"title":"EMLight: Lighting Estimation via Spherical Distribution Approximation","type":"publication"},{"authors":["Aoran Xiao","Xiaofei Yang","Shijian Lu","Dayan Guan","Jiaxing Huang"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"b8299e911c1932c33d6179a118b928c7","permalink":"https://sg-vilab.github.io/publication/xiao2021fps/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/xiao2021fps/","section":"publication","summary":"Scene understanding based on LiDAR point cloud is an essential task for autonomous cars to drive safely, which often employs spherical projection to map 3D point cloud into multi-channel 2D images for semantic segmentation. Most existing methods simply stack different point attributes/modalities (e.g. coordinates, intensity, depth, etc.) as image channels to increase information capacity, but ignore distinct characteristics of point attributes in different image channels. We design FPS-Net, a convolutional fusion network that exploits the uniqueness and discrepancy among the projected image channels for optimal point cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead of simply stacking multiple channel images as a single input, we group them into different modalities to first learn modality-specific features separately and then map the learned features into a common high-dimensional feature space for pixel-level fusion and learning. Specifically, we design a residual dense block with multiple receptive fields as a building block in the encoder which preserves detailed information in each modality and learns hierarchical modality-specific and fused features effectively. In the FPS-Net decoder, we use a recurrent convolution block likewise to hierarchically decode fused features into output space for pixel-level classification. Extensive experiments conducted on two widely adopted point cloud datasets show that FPS-Net achieves superior semantic segmentation as compared with state-of-the-art projection-based methods. In addition, the proposed modality fusion idea is compatible with typical projection-based methods and can be incorporated into them with consistent performance improvements.","tags":null,"title":"FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation","type":"publication"},{"authors":["Mengxi Jia","Xinhua Cheng","Yunpeng Zhai","Shijian Lu","Siwei Ma","Yonghong Tian","Jian Zhang"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"81c727429722deacd6f7ef432d5012ff","permalink":"https://sg-vilab.github.io/publication/jia2021matching/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/jia2021matching/","section":"publication","summary":"Occluded person re-identification (re-ID) is a challenging task as different human parts may become invisible in cluttered scenes, making it hard to match person images of different identities. Most existing methods address this challenge by aligning spatial features of body parts according to semantic information (e.g. human poses) or feature similarities but this approach is complicated and sensitive to noises. This paper presents Matching on Sets (MoS), a novel method that positions occluded person re-ID as a set matching task without requiring spatial alignment. MoS encodes a person image by a pattern set as represented by a `global vector’ with each element capturing one specific visual pattern, and it introduces Jaccard distance as a metric to compute the distance between pattern sets and measure image similarity. To enable Jaccard distance over continuous real numbers, we employ minimization and maximization to approximate the operations of intersection and union, respectively. In addition, we design a Jaccard triplet loss that enhances the pattern discrimination and allows to embed set matching into deep neural networks for end-to-end training. In the inference stage, we introduce a conflict penalty mechanism that detects mutually exclusive patterns in the pattern union of image pairs and decreases their similarities accordingly. Extensive experiments over three widely used datasets (Market1501, DukeMTMC and Occluded-DukeMTMC) show that MoS achieves superior re-ID performance. Additionally, it is tolerant of occlusions and outperforms the state-of-the-art by large margins for Occluded-DukeMTMC.","tags":null,"title":"Matching on Sets: Conquer Occluded Person Re-identification Without Alignment","type":"publication"},{"authors":null,"categories":null,"content":" FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation   Brain MRI super-resolution using coupled-projection residual network, Neurocomputing ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"2a642930ca3f5c8605a7e91e55eeb0db","permalink":"https://sg-vilab.github.io/post/ispr-neurocomputing2021/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/post/ispr-neurocomputing2021/","section":"post","summary":" FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation   Brain MRI super-resolution using coupled-projection residual network, Neurocomputing ","tags":null,"title":"Two Papers Accepted by Neurocomputing and ISPRS 2021","type":"post"},{"authors":null,"categories":null,"content":" Matching on Sets: Conquer Occluded Person Re-identification Without Alignment  EMLight: Lighting Estimation via Spherical Distribution Approximation ","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"eb69a5bf8f6a69ce83cae071272a81d8","permalink":"https://sg-vilab.github.io/post/aaai2021/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/post/aaai2021/","section":"post","summary":" Matching on Sets: Conquer Occluded Person Re-identification Without Alignment  EMLight: Lighting Estimation via Spherical Distribution Approximation ","tags":null,"title":"Two Papers Accepted by AAAI 2021","type":"post"},{"authors":["Mengxi Jia","Yunpeng Zhai","Shijian Lu","Siwei Ma","Jian Zhang"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"e2ac3bedf552346a32f63b7729af9075","permalink":"https://sg-vilab.github.io/publication/jia2020similarity/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/jia2020similarity/","section":"publication","summary":"RGB-Infrared (IR) cross-modality person re-identification (re-ID), which aims to search an IR image in RGB gallery or vice versa, is a challenging task due to the large discrepancy between IR and RGB modalities. Existing methods address this challenge typically by aligning feature distributions or image styles across modalities, whereas the very useful similarities among gallery samples of the same modality (i.e. intra-modality sample similarities) is largely neglected. This paper presents a novel similarity inference metric (SIM) that exploits the intra-modality sample similarities to circumvent the cross-modality discrepancy targeting optimal cross-modality image matching. SIM works by successive similarity graph reasoning and mutual nearest-neighbor reasoning that mine cross-modality sample similarities by leveraging intra-modality sample similarities from two different perspectives. Extensive experiments over two cross-modality re-ID datasets (SYSU-MM01 and RegDB) show that SIM achieves significant accuracy improvement but with little extra training as compared with the state-of-the-art.","tags":null,"title":"A Similarity Inference Metric for RGB-Infrared Cross-Modality Person Re-identification","type":"publication"},{"authors":["Xiaobing Zhang","Shijian Lu","Haigang Gong","Zhipeng Luo","Ming Liu"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"28872c2c657af965b14eb5f6dc4b96ff","permalink":"https://sg-vilab.github.io/publication/zhang2020amln/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/zhang2020amln/","section":"publication","summary":"Online knowledge distillation has attracted increasing interest recently, which jointly learns teacher and student models or an ensemble of student models simultaneously and collaboratively. On the other hand, existing works focus more on outcome-driven learning according to knowledge like classification probabilities whereas the distilling processes which capture rich and useful intermediate features and information are largely neglected. In this work, we propose an innovative adversarial-based mutual learning network (AMLN) that introduces process-driven learning beyond outcome-driven learning for augmented online knowledge distillation. A block-wise training module is designed which guides the information flow and mutual learning among peer networks adversarially throughout different learning stages, and this spreads until the final network layer which captures more high-level information. AMLN has been evaluated under a variety of network architectures over three widely used benchmark datasets. Extensive experiments show that AMLN achieves superior performance consistently against state-of-the-art knowledge transfer methods.","tags":null,"title":"AMLN: Adversarial-based Mutual Learning Network for Online Knowledge Distillation","type":"publication"},{"authors":["Siyuan Yang","Jun Liu","Shijian Lu","Meng Hwa Er","Alex C. Kot"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"f64558006bb95e0754ab9ca4862fc32a","permalink":"https://sg-vilab.github.io/publication/yang2020collaborative/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/yang2020collaborative/","section":"publication","summary":"Gesture recognition and 3D hand pose estimation are two highly correlated tasks, yet they are often handled separately. In this paper, we present a novel collaborative learning network for joint gesture recognition and 3D hand pose estimation. The proposed network exploits joint-aware features that are crucial for both tasks, with which gesture recognition and 3D hand pose estimation boost each other to learn highly discriminative features and models. In addition, a novel multi-order feature analysis method is introduced which learns posture and multi-order motion information from the intermediate feature maps of videos effectively and efficiently. Due to the exploitation of joint-aware features in common, the proposed technique is capable of learning gesture recognition and 3D hand pose estimation even when only gesture or pose labels are available, and this enables weakly supervised network learning with much reduced data labeling efforts. Extensive experiments show that our proposed method achieves superior gesture recognition and 3D hand pose estimation performance as compared with the state-of-the-art. Codes and models will be released upon the paper acceptance. \"","tags":null,"title":"Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-Order Feature Analysis","type":"publication"},{"authors":["Jiaxing Huang","Shijian Lu","Dayan Guan","Xiaobing Zhang"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"3014d78d015102255a3b15fafba27b52","permalink":"https://sg-vilab.github.io/publication/huang2020contextual/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/huang2020contextual/","section":"publication","summary":"Recent advances in unsupervised domain adaptation for semantic segmentation have shown great potentials to relieve the demand of expensive per-pixel annotations. However, most existing works address the domain discrepancy by aligning the data distributions of two domains at a global image level whereas the local consistencies are largely neglected. This paper presents an innovative local contextual-relation consistent domain adaptation (CrCDA) technique that aims to achieve local-level consistencies during the global-level alignment. The idea is to take a closer look at region-wise feature representations and align them for local-level consistencies. Specifically, CrCDA learns and enforces the prototypical local contextual-relations explicitly in the feature space of a labelled source domain while transferring them to an unlabelled target domain via back propagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is designed to optimally align these hundreds of local contextual-relations across domain without requiring discriminator or extra computation overhead. The proposed CrCDA has been evaluated extensively over two challenging domain adaptive segmentation tasks (e.g., GTA5 to Cityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior segmentation performance as compared with state-of-the-art methods.","tags":null,"title":"Contextual-Relation Consistent Domain Adaptation for Semantic Segmentation","type":"publication"},{"authors":null,"categories":null,"content":" Multiple Expert Brainstorming for Domain Adaptive Person Re-identification  LEED: Label-Free Expression Editing via Disentanglement  Contextual-Relation Consistent Domain Adaptation for Semantic Segmentation  Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-Order Feature Analysis [Spotlight]  AMLN: Adversarial-based Mutual Learning Network for Online Knowledge Distillation  ","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"12bac31a3a0180c0f77b3a2773253ed8","permalink":"https://sg-vilab.github.io/post/eccv2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/post/eccv2020/","section":"post","summary":" Multiple Expert Brainstorming for Domain Adaptive Person Re-identification  LEED: Label-Free Expression Editing via Disentanglement  Contextual-Relation Consistent Domain Adaptation for Semantic Segmentation  Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-Order Feature Analysis [Spotlight]  AMLN: Adversarial-based Mutual Learning Network for Online Knowledge Distillation  ","tags":null,"title":"Five Papers (One Spotlight) Accepted by ECCV 2020","type":"post"},{"authors":["Rongliang Wu","Shijian Lu"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"c04b5d2476f5fd46322fdb4b40e61c1e","permalink":"https://sg-vilab.github.io/publication/wu2020leed/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/wu2020leed/","section":"publication","summary":"Recent studies on facial expression editing have obtained very promising progress. On the other hand, existing methods face the constraint of requiring a large amount of expression labels which are often expensive and time-consuming to collect. This paper presents an innovative label-free expression editing via disentanglement (LEED) framework that is capable of editing the expression of both frontal and profile facial images without requiring any expression label. The idea is to disentangle the identity and expression of a facial image in the expression manifold, where the neutral face captures the identity attribute and the displacement between the neutral image and the expressive image captures the expression attribute. Two novel losses are designed for optimal expression disentanglement and consistent synthesis, including a mutual expression information loss that aims to extract pure expression-related features and a siamese loss that aims to enhance the expression similarity between the synthesized image and the reference image. Extensive experiments over two public facial expression datasets show that LEED achieves superior facial expression editing qualitatively and quantitatively.","tags":null,"title":"LEED: Label-Free Expression Editing via Disentanglement","type":"publication"},{"authors":["Yunpeng Zhai","Qixiang Ye","Shijian Lu","Mengxi Jia","Rongrong Ji","Yonghong Tian"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"0124e707f4d57ec39deb9d641d0532e0","permalink":"https://sg-vilab.github.io/publication/zhai2020multiple/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/zhai2020multiple/","section":"publication","summary":"Often the best performing deep neural models are ensembles of multiple base-level networks, nevertheless, ensemble learning with respect to domain adaptive person re-ID remains unexplored. In this paper, we propose a multiple expert brainstorming network (MEB-Net) for domain adaptive person re-ID, opening up a promising direction about model ensemble problem under unsupervised conditions. MEB-Net adopts a mutual learning strategy, where multiple networks with different architectures are pre-trained within a source domain as expert models equipped with specific features and knowledge, while the adaptation is then accomplished through brainstorming (mutual learning) among expert models. MEB-Net accommodates the heterogeneity of experts learned with different architectures and enhances discrimination capability of the adapted re-ID model, by introducing a regularization scheme about authority of experts. Extensive experiments on large-scale datasets (Market-1501 and DukeMTMC-reID) demonstrate the superior performance of MEB-Net over the state-of-the-arts.","tags":null,"title":"Multiple Expert Brainstorming for Domain Adaptive Person Re-identification","type":"publication"},{"authors":null,"categories":null,"content":" A Similarity Inference Metric for RGB-Infrared Cross-Modality Person Re-identification ","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"02b043ad060bcda1b2aee40ac92949f3","permalink":"https://sg-vilab.github.io/post/ijcai2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/post/ijcai2020/","section":"post","summary":" A Similarity Inference Metric for RGB-Infrared Cross-Modality Person Re-identification ","tags":null,"title":"One Paper Accepted by IJCAI 2020","type":"post"},{"authors":["Yunpeng Zhai","Shijian Lu","Qixiang Ye","Xuebo Shan","Jie Chen","Rongrong Ji","Yonghong Tian"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"af78a8c4da3b2fbc15231b217086b676","permalink":"https://sg-vilab.github.io/publication/zhai2020ad/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/zhai2020ad/","section":"publication","summary":"Domain adaptive person re-identification (re-ID) is a challenging task, especially when person identities in target domains are unknown. Existing methods attempt to address this challenge by transferring image styles or aligning feature distributions across domains, whereas the rich unlabeled samples in target domains are not sufficiently exploited. This paper presents a novel augmented discriminative clustering (AD-Cluster) technique that estimates and augments person clusters in target domains and enforces the discrimination ability of re-ID models with the augmented clusters. AD-Cluster is trained by iterative density-based clustering, adaptive sample augmentation, and discriminative feature learning. It learns an image generator and a feature encoder which aim to maximize the intra-cluster diversity in the sample space and minimize the intra-cluster distance in the feature space in an adversarial min-max manner. Finally, AD-Cluster increases the diversity of sample clusters and improves the discrimination capability of re-ID models greatly. Extensive experiments over Market-1501 and DukeMTMC-reID show that AD-Cluster outperforms the state-of-the-art with large margins.","tags":null,"title":"AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive Person Re-identification","type":"publication"},{"authors":["Rongliang Wu","Gongjie Zhang","Shijian Lu","Tao Chen"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"0047f8e03cb27ddb870677a52cab549b","permalink":"https://sg-vilab.github.io/publication/wu2020cascade/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/wu2020cascade/","section":"publication","summary":"Recent advances in Generative Adversarial Nets (GANs) have shown remarkable improvements for facial expression editing. However, current methods are still prone to generate artifacts and blurs around expression-intensive regions, and often introduce undesired overlapping artifacts while handling large-gap expression transformations such as transformation from furious to laughing. To address these limitations, we propose Cascade Expression Focal GAN (Cascade EF-GAN), a novel network that performs progressive facial expression editing with local expression focuses. The introduction of the local focus enables the Cascade EF-GAN to better preserve identity-related features and details around eyes, noses and mouths, which further helps reduce artifacts and blurs within the generated facial images. In addition, an innovative cascade transformation strategy is designed by dividing a large facial expression transformation into multiple small ones in cascade, which helps suppress overlapping artifacts and produce more realistic editing while dealing with large-gap expression transformations. Extensive experiments over two publicly available facial expression datasets show that our proposed Cascade EF-GAN achieves superior performance for facial expression editing.","tags":null,"title":"Cascade EF-GAN: Progressive Facial Expression Editing with Local Focuses","type":"publication"},{"authors":null,"categories":null,"content":" Synergistic 2D/3D Convolutional Neural Network for Hyperspectral Image Classification ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"b84824e0ddaf39e8e1bfa89044d7efad","permalink":"https://sg-vilab.github.io/post/remote2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/post/remote2020/","section":"post","summary":" Synergistic 2D/3D Convolutional Neural Network for Hyperspectral Image Classification ","tags":null,"title":"One Paper Accepted by Remote Sensing 2020","type":"post"},{"authors":["Kai Wang","Xiaojiang Peng","Jianfei Yang","Shijian Lu","Yu Qiao"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"e32754a121281181bf8da8cdafe59954","permalink":"https://sg-vilab.github.io/publication/wang2020suppressing/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/wang2020suppressing/","section":"publication","summary":"Annotating a qualitative large-scale facial expression dataset is extremely difficult due to the uncertainties caused by ambiguous facial expressions, low-quality facial images, and the subjectiveness of annotators. These uncertainties lead to a key challenge of large-scale Facial Expression Recognition (FER) in deep learning era. To address this problem, this paper proposes a simple yet efficient Self-Cure Network (SCN) which suppresses the uncertainties efficiently and prevents deep networks from over-fitting uncertain facial images. Specifically, SCN suppresses the uncertainty from two different aspects':' 1) a self-attention mechanism over mini-batch to weight each training sample with a ranking regularization, and 2) a careful relabeling mechanism to modify the labels of these samples in the lowest-ranked group. Experiments on synthetic FER datasets and our collected WebEmotion dataset validate the effectiveness of our method. Results on public benchmarks demonstrate that our SCN outperforms current state-of-the-art methods with \\textbf{88.14}\\% on RAF-DB, \\textbf{60.23}\\% on AffectNet, and \\textbf{89.35}\\% on FERPlus.","tags":null,"title":"Suppressing Uncertainties for Large-Scale Facial Expression Recognition","type":"publication"},{"authors":["Xiaofei Yang","Xiaofeng Zhang","Yunming Ye","Raymond Y. K. Lau","Shijian Lu","Xutao Li","Xiaohui Huang"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"b57b416f7794eb9fad780d2c7badf9cb","permalink":"https://sg-vilab.github.io/publication/yang2020synergistic/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/yang2020synergistic/","section":"publication","summary":"Accurate hyperspectral image classification has been an important yet challenging task for years. With the recent success of deep learning in various tasks, 2-dimensional (2D)/3-dimensional (3D) convolutional neural networks (CNNs) have been exploited to capture spectral or spatial information in hyperspectral images. On the other hand, few approaches make use of both spectral and spatial information simultaneously, which is critical to accurate hyperspectral image classification. This paper presents a novel Synergistic Convolutional Neural Network (SyCNN) for accurate hyperspectral image classification. The SyCNN consists of a hybrid module that combines 2D and 3D CNNs in feature learning and a data interaction module that fuses spectral and spatial hyperspectral information. Additionally, it introduces a 3D attention mechanism before the fully-connected layer which helps filter out interfering features and information effectively. Extensive experiments over three public benchmarking datasets show that our proposed SyCNNs clearly outperform state-of-the-art techniques that use 2D/3D CNNs.","tags":null,"title":"Synergistic 2D/3D Convolutional Neural Network for Hyperspectral Image Classification","type":"publication"},{"authors":null,"categories":null,"content":" Suppressing Uncertainties for Large-Scale Facial Expression Recognition  Cascade EF-GAN: Progressive Facial Expression Editing with Local Focuses  AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive Person Re-identification ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"2a1ebe5f5d97651e0aa20fec56a13961","permalink":"https://sg-vilab.github.io/post/cvpr2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/post/cvpr2020/","section":"post","summary":" Suppressing Uncertainties for Large-Scale Facial Expression Recognition  Cascade EF-GAN: Progressive Facial Expression Editing with Local Focuses  AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive Person Re-identification ","tags":null,"title":"Three Papers Accepted by CVPR 2020","type":"post"},{"authors":["Fan Yang","Ke Yan","Shijian Lu","Huizhu Jia","Don Xie","Zongqiao Yu","Xiaowei Guo","Feiyue Huang","Wen Gao"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"35bbbe27336f4f32fc0cf6edc4f16599","permalink":"https://sg-vilab.github.io/publication/yang2020part/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/yang2020part/","section":"publication","summary":"Unsupervised domain adaptation (UDA) aims to mitigate the domain shift that occurs when transferring knowledge from a labeled source domain to an unlabeled target domain. While it has been studied for application in unsupervised person re-identification (ReID), the relations of feature distribution across the source and target domains remain underexplored, as they either ignore the local relations or omit the in-depth consideration of negative transfer when two domains do not share identical label spaces. In light of the above, this paper presents an innovative part-aware progressive adaptation network (PPAN) that exploits global and local relations for UDA-based ReID across domains. A multi-branch network is developed that explicitly learns discriminative feature representation from both whole-body images and body-part images under the supervision of a labeled source domain. Within each network branch, an independent UDA constraint is designed that aligns the global and local feature distributions from a labeled source domain with those of an unlabeled target domain. In addition, a novel progressive adaptation strategy (PAS) is designed that effectively alleviates the negative influence of outlier source identities. The proposed unsupervised ReID model is evaluated on five widely used datasets (Market-1501, DukeMTMC-reID, CUHK03, VIPeR and PRID), and experimental results demonstrate its superior robustness and effectiveness relative to state-of-the-art approaches.","tags":null,"title":"Part-aware Progressive Unsupervised Domain Adaptation for Person Re-Identification","type":"publication"},{"authors":["Qinghua Ren","Shijian Lu","Jinxia Zhang","Renjie Hu"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"5e1f5a21bb8a09849e14b0dfcff79c4c","permalink":"https://sg-vilab.github.io/publication/ren2020salient/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/ren2020salient/","section":"publication","summary":"Benefiting from the powerful discriminative feature learning capability of convolutional neural networks (CNNs), deep learning techniques have achieved remarkable performance improvement for the task of salient object detection (SOD) in recent years. However, most existing deep SOD models do not fully exploit informative contextual features, which often leads to suboptimal detection performance in the presence of a cluttered background. This paper presents a context-aware attention module that detects salient objects by simultaneously constructing connections between each image pixel and its local and global contextual pixels. Specifically, each pixel and its neighbors bidirectionally exchange semantic information by computing their correlation coefficients, and this process aggregates contextual attention features both locally and globally. In addition, an attention-guided hierarchical network architecture is designed to capture fine-grained spatial details by transmitting contextual information from deeper to shallower network layers in a top-down manner. Extensive experiments on six public SOD datasets show that our proposed model demonstrates superior SOD performance against most of the current state-of-the-art models under different evaluation metrics.","tags":null,"title":"Salient Object Detection by Fusing Local and Global Contexts","type":"publication"},{"authors":null,"categories":null,"content":" Salient Object Detection by Fusing Local and Global Contexts  Part-aware Progressive Unsupervised Domain Adaptation for Person Re-Identification ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"7f01e3b770b8fa8f73b0306569c69b35","permalink":"https://sg-vilab.github.io/post/tmm2020/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/post/tmm2020/","section":"post","summary":" Salient Object Detection by Fusing Local and Global Contexts  Part-aware Progressive Unsupervised Domain Adaptation for Person Re-Identification ","tags":null,"title":"Two Papers Accepted by TMM 2020","type":"post"},{"authors":null,"categories":null,"content":" Single-Image Dehazing via Compositional Adversarial Network ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"738c2f1a3ec544cdc1d1f9b094c0d386","permalink":"https://sg-vilab.github.io/post/tcyb2019/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/post/tcyb2019/","section":"post","summary":" Single-Image Dehazing via Compositional Adversarial Network ","tags":null,"title":"One Paper Accepted by TCYB 2019","type":"post"},{"authors":["Hongyuan Zhu","Yi Cheng","Xi Peng","Joey Tianyi Zhou","Zhao Kang","Shijian Lu","Zhiwen Fang","Liyuan Li","Joo-Hwee Lim"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"0bc7ea63d14de2bf758f711bd8fa660f","permalink":"https://sg-vilab.github.io/publication/zhu2019single/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/zhu2019single/","section":"publication","summary":"Single-image dehazing has been an important topic given the commonly occurred image degradation caused by adverse atmosphere aerosols. The key to haze removal relies on an accurate estimation of global air-light and the transmission map. Most existing methods estimate these two parameters using separate pipelines which reduces the efficiency and accumulates errors, thus leading to a suboptimal approximation, hurting the model interpretability, and degrading the performance. To address these issues, this article introduces a novel generative adversarial network (GAN) for single-image dehazing. The network consists of a novel compositional generator and a novel deeply supervised discriminator. The compositional generator is a densely connected network, which combines fine-scale and coarse-scale information. Benefiting from the new generator, our method can directly learn the physical parameters from data and recover clean images from hazy ones in an end-to-end manner. The proposed discriminator is deeply supervised, which enforces that the output of the generator to look similar to the clean images from low-level details to high-level structures. To the best of our knowledge, this is the first end-to-end generative adversarial model for image dehazing, which simultaneously outputs clean images, transmission maps, and air-lights. Extensive experiments show that our method remarkably outperforms the state-of-the-art methods. Furthermore, to facilitate future research, we create the HazeCOCO dataset which is currently the largest dataset for single-image dehazing.","tags":null,"title":"Single-Image Dehazing via Compositional Adversarial Network","type":"publication"},{"authors":["Yuechen Wu","Zhenhuan Rao","Wei Zhang","Shijian Lu","Weizhi Lu","Zheng-Jun Zha"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"62e5e4297f2fcba402b3bfae7f6efaf6","permalink":"https://sg-vilab.github.io/publication/wu2019exploring/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/wu2019exploring/","section":"publication","summary":"Learning to adapt to a series of different goals in visual navigation is challenging. In this work, we present a model-embedded actor-critic architecture for the multi-goal visual navigation task. To enhance the task cooperation in multi-goal learning, we introduce two new designs to the reinforcement learning scheme':' inverse dynamics model (InvDM) and multi-goal co-learning (MgCl). Specifically, InvDM is proposed to capture the navigation-relevant association between state and goal, and provide additional training signals to relieve the sparse reward issue. MgCl aims at improving the sample efficiency and supports the agent to learn from unintentional positive experiences. Extensive results on the interactive platform AI2-THOR demonstrate that the proposed method converges faster than state-of-the-art methods while producing more direct routes to navigate to the goal. The video demonstration is available at':' https://youtube.com/channel/ UCtpTMOsctt3yPzXqe JMD3w/videos.","tags":null,"title":"Exploring the Task Cooperation in Multi-goal Visual Navigation","type":"publication"},{"authors":["Fangneng Zhan","Chuhui Xue","Shijian Lu"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"cb5d9847a8ed72acb198d964709c2883","permalink":"https://sg-vilab.github.io/publication/zhan2019ga/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/zhan2019ga/","section":"publication","summary":"Recent adversarial learning research has achieved very impressive progress for modelling cross-domain data shifts in appearance space but its counterpart in modelling cross-domain shifts in geometry space lags far behind. This paper presents an innovative Geometry-Aware Domain Adaptation Network (GA-DAN) that is capable of modelling cross-domain shifts concurrently in both geometry space and appearance space and realistically converting images across domains with very different characteristics. In the proposed GA-DAN, a novel multi-modal spatial learning technique is designed which converts a source-domain image into multiple images of different spatial views as in the target domain. A new disentangled cycle-consistency loss is introduced which balances the cycle consistency in appearance and geometry spaces and improves the learning of the whole network greatly. The proposed GA-DAN has been evaluated for the classic scene text detection and recognition tasks, and experiments show that the domain-adapted images achieve superior scene text detection and recognition performance while applied to network training.","tags":null,"title":"GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition","type":"publication"},{"authors":["Chuhui Xue","Shijian Lu","Wei Zhang"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"103c63e25553ed3087f530fbd03cd9ec","permalink":"https://sg-vilab.github.io/publication/xue2019msr/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/xue2019msr/","section":"publication","summary":"State-of-the-art scene text detection techniques predict quadrilateral boxes that are prone to localization errors while dealing with straight or curved text lines of different orientations and lengths in scenes. This paper presents a novel multi-scale shape regression network (MSR) that is capable of locating text lines of different lengths, shapes and curvatures in scenes. The proposed MSR detects scene texts by predicting dense text boundary points that inherently capture the location and shape of text lines accurately and are also more tolerant to the variation of text line length as compared with the state of the arts using proposals or segmentation. Additionally, the multi-scale network extracts and fuses features at different scales which demonstrates superb tolerance to the text scale variation. Extensive experiments over several public datasets show that the proposed MSR obtains superior detection performance for both curved and straight text lines of different lengths and orientations.","tags":null,"title":"MSR: Multi-Scale Shape Regression for Scene Text Detection","type":"publication"},{"authors":null,"categories":null,"content":" GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition, ICCV  MSR: Multi-Scale Shape Regression for Scene Text Detection, IJCAI  Exploring the Task Cooperation in Multi-goal Visual Navigation, IJCAI ","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"d4fad5a93e91a732d82365d7e2189b6c","permalink":"https://sg-vilab.github.io/post/ijcai-iccv2019/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/post/ijcai-iccv2019/","section":"post","summary":" GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition, ICCV  MSR: Multi-Scale Shape Regression for Scene Text Detection, IJCAI  Exploring the Task Cooperation in Multi-goal Visual Navigation, IJCAI ","tags":null,"title":"Three Papers Accepted by ICCV and IJCAI 2019","type":"post"},{"authors":["Fangneng Zhan","Shijian Lu"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"961a7865d580aba5e259653daabe3cca","permalink":"https://sg-vilab.github.io/publication/zhan2019esir/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/zhan2019esir/","section":"publication","summary":"Automated recognition of texts in scenes has been a research challenge for years, largely due to the arbitrary variation of text appearances in perspective distortion, text line curvature, text styles and different types of imaging artifacts. The recent deep networks are capable of learning robust representations with respect to imaging artifacts and text style changes, but still face various problems while dealing with scene texts with perspective and curvature distortions. This paper presents an end-to-end trainable scene text recognition system (ESIR) that iteratively removes perspective distortion and text line curvature as driven by better scene text recognition performance. An innovative rectification network is developed which employs a novel line-fitting transformation to estimate the pose of text lines in scenes. In addition, an iterative rectification pipeline is developed where scene text distortions are corrected iteratively towards a fronto-parallel view. The ESIR is also robust to parameter initialization and the training needs only scene text images and word-level annotations as required by most scene text recognition systems. Extensive experiments over a number of public datasets show that the proposed ESIR is capable of rectifying scene text distortions accurately, achieving superior recognition performance for both normal scene text images and those suffering from perspective and curvature distortions.","tags":null,"title":"ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification","type":"publication"},{"authors":["Fangneng Zhan","Hongyuan Zhu","Shijian Lu"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"b997508cfa156ab2f5a6497417ae40ba","permalink":"https://sg-vilab.github.io/publication/zhan2019spatial/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/zhan2019spatial/","section":"publication","summary":"Recent advances in generative adversarial networks (GANs) have shown great potentials in realistic image synthesis whereas most existing works address synthesis realism in either appearance space or geometry space but few in both. This paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a geometry synthesizer and an appearance synthesizer to achieve synthesis realism in both geometry and appearance spaces. The geometry synthesizer learns contextual geometries of background images and transforms and places foreground objects into the background images unanimously. The appearance synthesizer adjusts the color, brightness and styles of the foreground objects and embeds them into background images harmoniously, where a guided filter is introduced for detail preserving. The two synthesizers are inter-connected as mutual references which can be trained end-to-end without supervision. The SF-GAN has been evaluated in two tasks':' (1) realistic scene text image synthesis for training better recognition models; (2) glass and hat wearing for realistic matching glasses and hats with real portraits. Qualitative and quantitative comparisons with the state-of-the-art demonstrate the superiority of the proposed SF-GAN.","tags":null,"title":"Spatial Fusion GAN for Image Synthesis","type":"publication"},{"authors":null,"categories":null,"content":" Towards Natural and Accurate Future Motion Prediction of Humans and Animals  Spatial Fusion GAN for Image Synthesis  ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"c5de97d83a168698eb63deb775befd65","permalink":"https://sg-vilab.github.io/post/cvpr2019/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/post/cvpr2019/","section":"post","summary":" Towards Natural and Accurate Future Motion Prediction of Humans and Animals  Spatial Fusion GAN for Image Synthesis  ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification ","tags":null,"title":"Three Papers Accepted by CVPR 2019","type":"post"},{"authors":["Zhenguang Liu","Shuang Wu","Shuyuan Jin","Qi Liu","Shijian Lu","Roger Zimmermann","Li Cheng"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"95fa6d2980c416bfc6321a4c2bb1d922","permalink":"https://sg-vilab.github.io/publication/liu2019towards/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/liu2019towards/","section":"publication","summary":"Anticipating the future motions of 3D articulate objects is challenging due to its non-linear and highly stochastic nature. Current approaches typically represent the skeleton of an articulate object as a set of 3D joints, which unfortunately ignores the relationship between joints, and fails to encode fine-grained anatomical constraints. Moreover, conventional recurrent neural networks, such as LSTM and GRU, are employed to model motion contexts, which inherently have difficulties in capturing long-term dependencies. To address these problems, we propose to explicitly encode anatomical constraints by modeling their skeletons with a Lie algebra representation. Importantly, a hierarchical recurrent network structure is developed to simultaneously encodes local contexts of individual frames and global contexts of the sequence. We proceed to explore the applications of our approach to several distinct quantities including human, fish, and mouse. Extensive experiments show that our approach achieves more natural and accurate predictions over state-of-the-art methods.","tags":null,"title":"Towards Natural and Accurate Future Motion Prediction of Humans and Animals","type":"publication"},{"authors":null,"categories":null,"content":" SS-HCNN:Semi-Supervised Hierarchical Convolutional Neural Network for Image Classification ","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"83896922fbdd72a21af95121afd407f9","permalink":"https://sg-vilab.github.io/post/tip2019/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/post/tip2019/","section":"post","summary":" SS-HCNN:Semi-Supervised Hierarchical Convolutional Neural Network for Image Classification ","tags":null,"title":"One Paper Accepted by TIP 2019","type":"post"},{"authors":["Tao Chen","Shijian Lu","Jiayuan Fan"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"a7b0ece5488f42a9ebf7fc93e004ba41","permalink":"https://sg-vilab.github.io/publication/chen2019ss/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/publication/chen2019ss/","section":"publication","summary":"The availability of large-scale annotated data and the uneven separability of different data categories have become two major impediments of deep learning for image classification. In this paper, we present a semi-supervised hierarchical convolutional neural network (SS-HCNN) to address these two challenges. A large-scale unsupervised maximum margin clustering technique is designed, which splits images into a number of hierarchical clusters iteratively to learn cluster-level CNNs at parent nodes and category-level CNNs at leaf nodes. The splitting uses the similarity of CNN features to group visually similar images into the same cluster, which relieves the uneven data separability constraint. With the hierarchical cluster-level CNNs capturing certain high-level image category information, the category-level CNNs can be trained with a small amount of labeled images, and this relieves the data annotation constraint. A novel cluster splitting criterion is also designed, which automatically terminates the image clustering in the tree hierarchy. The proposed SS-HCNN has been evaluated on the CIFAR-100 and ImageNet classification datasets. The experiments show that the SS-HCNN trained using a portion of labeled training images can achieve comparable performance with other fully trained CNNs using all labeled images. Additionally, the SS-HCNN trained using all labeled images clearly outperforms other fully trained CNNs.","tags":null,"title":"SS-HCNN:Semi-Supervised Hierarchical Convolutional Neural Network for Image Classification","type":"publication"},{"authors":["Dinh Nguyen Van","Shijian Lu","Shangxuan Tian","Nizar Ouarti","Mounir Mokhtari"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"2b79b41de75fb96ad187ca478c47a8d9","permalink":"https://sg-vilab.github.io/publication/nguyenvan2019pooling/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/nguyenvan2019pooling/","section":"publication","summary":"Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.","tags":null,"title":"A pooling based scene text proposal technique for scene text reading in the wild","type":"publication"},{"authors":["Gongjie Zhang","Shijian Lu","Wei Zhang"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"109dc77dec4a2dc50e58af0c3472b1fd","permalink":"https://sg-vilab.github.io/publication/zhang2019cad/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/zhang2019cad/","section":"publication","summary":"Accurate and robust detection of multi-class objects in optical remote sensing images is essential to many real-world applications such as urban planning, traffic control, searching and rescuing, etc. However, state-of-the-art object detection techniques designed for images captured using ground-level sensors usually experience a sharp performance drop when directly applied to remote sensing images, largely due to the object appearance differences in remote sensing images in term of sparse texture, low contrast, arbitrary orientations, large scale variations, etc. This paper presents a novel object detection network (CAD-Net) that exploits attention-modulated features as well as global and local contexts to address the new challenges in detecting objects from remote sensing images. The proposed CAD-Net learns global and local contexts of objects by capturing their correlations with the global scene (at scene-level) and the local neighboring objects or features (at object-level), respectively. In addition, it designs a spatial-and-scale-aware attention module that guides the network to focus on more informative regions and features as well as more appropriate feature scales. Experiments over two publicly available object detection datasets for remote sensing images demonstrate that the proposed CAD-Net achieves superior detection performance. The implementation codes will be made publicly available for facilitating future researches.","tags":null,"title":"CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing Imagery","type":"publication"},{"authors":null,"categories":null,"content":" CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing Imagery ","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"99bf52b858f095bb24fcdc5f274f236d","permalink":"https://sg-vilab.github.io/post/tgrs2019/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/post/tgrs2019/","section":"post","summary":" CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing Imagery ","tags":null,"title":"One Paper Accepted by TGRS 2019","type":"post"},{"authors":null,"categories":null,"content":" A pooling based scene text proposal technique for scene text reading in the wild  Attention driven person re-identification ","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"4f6d8a2516c7a9a9ef88f5da841dc490","permalink":"https://sg-vilab.github.io/post/pr2019/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/post/pr2019/","section":"post","summary":" A pooling based scene text proposal technique for scene text reading in the wild  Attention driven person re-identification ","tags":null,"title":"Two Papers Accepted by Pattern Recognition 2019","type":"post"},{"authors":["Fan Yang","Ke Yan","Shijian Lu","Huizhu Jia","Xiaodong Xie","Wen Gao"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"50218331e024908f55d5ecdb463a8eed","permalink":"https://sg-vilab.github.io/publication/yang2019attention/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/yang2019attention/","section":"publication","summary":"Person re-identification (ReID) is a challenging task due to arbitrary human pose variations, background clutters, etc. It has been studied extensively in recent years, but the multifarious local and global features are still not fully exploited by either ignoring the interplay between whole-body images and body-part images or missing in-depth examination of specific body-part images. In this paper, we propose a novel attention-driven multi-branch network that learns robust and discriminative human representation from global whole-body images and local body-part images simultaneously. Within each branch, an intra-attention network is designed to search for informative and discriminative regions within the whole-body or body-part images, where attention is elegantly decomposed into spatial-wise attention and channel-wise attention for effective and efficient learning. In addition, a novel inter-attention module is designed which fuses the output of intra-attention networks adaptively for optimal person ReID. The proposed technique has been evaluated over three widely used datasets CUHK03, Market-1501 and DukeMTMC-ReID, and experiments demonstrate its superior robustness and effectiveness as compared with the state of the arts.","tags":null,"title":"Attention driven person re-identification","type":"publication"},{"authors":null,"categories":null,"content":" Superpixel Guided Deep-Sparse-Representation Learning for Hyperspectral Image Classification ","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"442d8b2b4f13df3f769ecf2ffa3caed6","permalink":"https://sg-vilab.github.io/post/tcsvt2018/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/post/tcsvt2018/","section":"post","summary":" Superpixel Guided Deep-Sparse-Representation Learning for Hyperspectral Image Classification ","tags":null,"title":"One Paper Accepted by TCSVT 2018","type":"post"},{"authors":["Jiayuan Fan","Tao Chen","Shijian Lu"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"9d719a701e5a1db2cdd36590302cd775","permalink":"https://sg-vilab.github.io/publication/fan2017superpixel/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/publication/fan2017superpixel/","section":"publication","summary":"This paper presents a new technique for hyperspectral image (HSI) classification by using superpixel guided deep-sparse-representation learning. The proposed technique constructs a hierarchical architecture by exploiting the sparse coding to learn the HSI representation. Specifically, a multiple-layer architecture using different superpixel maps is designed, where each superpixel map is generated by downsampling the superpixels gradually along with enlarged spatial regions for labeled samples. In each layer, sparse representation of pixels within every spatial region is computed to construct a histogram via the sum-pooling with l 1 normalization. Finally, the representations (features) learned from the multiple-layer network are aggregated and trained by a support vector machine classifier. The proposed technique has been evaluated over three public HSI data sets, including the Indian Pines image set, the Salinas image set, and the University of Pavia image set. Experiments show superior performance compared with the state-of-the-art methods.","tags":null,"title":"Superpixel Guided Deep-Sparse-Representation Learning for Hyperspectral Image Classification","type":"publication"},{"authors":null,"categories":null,"content":" S-CNN: Subcategory-Aware Convolutional Networks for Object Detection ","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"57dd0059be2fcee59f88ce9753ffacde","permalink":"https://sg-vilab.github.io/post/tpami2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/post/tpami2018/","section":"post","summary":" S-CNN: Subcategory-Aware Convolutional Networks for Object Detection ","tags":null,"title":"One Paper Accepted by TPAMI 2018","type":"post"},{"authors":["Tao Chen","Shijian Lu","Jiayuan Fan"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"06dc3249028525db43b9367deb9e1d90","permalink":"https://sg-vilab.github.io/publication/chen2017s/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/chen2017s/","section":"publication","summary":"The marriage between the deep convolutional neural network (CNN) and region proposals has made breakthroughs for object detection in recent years. While the discriminative object features are learned via a deep CNN for classification, the large intra-class variation and deformation still limit the performance of the CNN based object detection. We propose a subcategory-aware CNN (S-CNN) to solve the object intra-class variation problem. In the proposed technique, the training samples are first grouped into multiple subcategories automatically through a novel instance sharing maximum margin clustering process. A multi-component Aggregated Channel Feature (ACF) detector is then trained to produce more latent training samples, where each ACF component corresponds to one clustered subcategory. The produced latent samples together with their subcategory labels are further fed into a CNN classifier to filter out false proposals for object detection. An iterative learning algorithm is designed for the joint optimization of image subcategorization, multi-component ACF detector, and subcategory-aware CNN classifier. Experiments on INRIA Person dataset, Pascal VOC 2007 dataset and MS COCO dataset show that the proposed technique clearly outperforms the state-of-the-art methods for generic object detection.","tags":null,"title":"S-CNN: Subcategory-Aware Convolutional Networks for Object Detection","type":"publication"},{"authors":["Chuhui Xue","Shijian Lu","Fangneng Zhan"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"f21e2664265d4cf484aa77c9d2221fe5","permalink":"https://sg-vilab.github.io/publication/xue2018accurate/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/xue2018accurate/","section":"publication","summary":"This paper presents a scene text detection technique that exploits bootstrapping and text border semantics for accurate localization of texts in scenes. A novel bootstrapping technique is designed which samples multiple 'subsections' of a word or text line and accordingly relieves the constraint of limited training data effectively. At the same time, the repeated sampling of text 'subsections' improves the consistency of the predicted text feature maps which is critical in predicting a single complete instead of multiple broken boxes for long words or text lines. In addition, a semantics-aware text border detection technique is designed which produces four types of text border segments for each scene text. With semantics-aware text borders, scene texts can be localized more accurately by regressing text pixels around the ends of words or text lines instead of all text pixels which often leads to inaccurate localization while dealing with long words or text lines. Extensive experiments demonstrate the effectiveness of the proposed techniques, and superior performance is obtained over several public datasets, e. g. 80.1 f-score for the MSRA-TD500, 67.1 f-score for the ICDAR2017-RCTW, etc.","tags":null,"title":"Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping","type":"publication"},{"authors":null,"categories":null,"content":" Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes  Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping ","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"708633b93d4e852cb58db94f1350f58d","permalink":"https://sg-vilab.github.io/post/eccv2018/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/post/eccv2018/","section":"post","summary":" Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes  Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping ","tags":null,"title":"Two Papers Accepted by ECCV 2018","type":"post"},{"authors":["Fangneng Zhan","Shijian Lu","Chuhui Xue"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"3e36d4fc417eb13d1b87e72799ebe1e8","permalink":"https://sg-vilab.github.io/publication/zhan2018verisimilar/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/zhan2018verisimilar/","section":"publication","summary":"The requirement of large amounts of annotated images has become one grand challenge while training deep neural network models for various visual detection and recognition tasks. This paper presents a novel image synthesis technique that aims to generate a large amount of annotated scene text images for training accurate and robust scene text detection and recognition models. The proposed technique consists of three innovative designs. First, it realizes \"semantic coherent\" synthesis by embedding texts at semantically sensible regions within the background image, where the semantic coherence is achieved by leveraging the semantic annotations of objects and image regions that have been created in the prior semantic segmentation research. Second, it exploits visual saliency to determine the embedding locations within each semantic sensible region, which coincides with the fact that texts are often placed around homogeneous regions for better visibility in scenes. Third, it designs an adaptive text appearance model that determines the color and brightness of embedded texts by learning from the feature of real scene text images adaptively. The proposed technique has been evaluated over five public datasets and the experiments show its superior performance in training accurate and robust scene text detection and recognition models.","tags":null,"title":"Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes","type":"publication"},{"authors":null,"categories":null,"content":" YoTube: Searching Action Proposal via Recurrent and Static Regression Networks ","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"8a61f87f7663f0b1b4272c3e5112062b","permalink":"https://sg-vilab.github.io/post/tip2018/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/post/tip2018/","section":"post","summary":" YoTube: Searching Action Proposal via Recurrent and Static Regression Networks ","tags":null,"title":"One Paper Accepted by TIP 2018","type":"post"},{"authors":["Hongyuan Zhu","Romain Vial","Shijian Lu","Yonghong Tian","Xianbin Cao"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"7895c9fe487a8a5e5ba09d8dec43fcf9","permalink":"https://sg-vilab.github.io/publication/zhu2018yotube/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/zhu2018yotube/","section":"publication","summary":"In this paper, we propose YoTube-a novel deep learning framework for generating action proposals in untrimmed videos, where each action proposal corresponds to a spatial-temporal tube that potentially locates one human action. Most of the existing works generate proposals by clustering low-level features or linking image proposals, which ignore the interplay between long-term temporal context and short-term cues. Different from these works, our method considers the interplay by designing a new recurrent YoTube detector and static YoTube detector. The recurrent YoTube detector sequentially regresses candidate bounding boxes using Recurrent Neural Network learned long-term temporal contexts. The static YoTube detector produces bounding boxes using rich appearance cues in every single frame. To fully exploit the complementary appearance, motion, and temporal context, we train the recurrent and static detector using RGB (Color) and flow information. Moreover, we fuse the corresponding outputs of the detectors to produce accurate and robust proposal boxes and obtain the final action proposals by linking the proposal boxes using dynamic programming with a novel path trimming method. Benefiting from the pipeline of our method, the untrimmed video could be effectively and efficiently handled. Extensive experiments on the challenging UCF-101, UCF-Sports, and JHMDB datasets show superior performance of the proposed method compared with the state of the arts.","tags":null,"title":"YoTube: Searching Action Proposal via Recurrent and Static Regression Networks","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://sg-vilab.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://sg-vilab.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://sg-vilab.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]